{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca302e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CNN Parameters: {'model__learning_rate': 0.001, 'model__kernel_size': 1, 'model__filters': 128, 'model__dense_units': 32, 'epochs': 50, 'batch_size': 16}\n",
      "Best LSTM Parameters: {'model__units': 50, 'model__learning_rate': 0.001, 'model__dense_units': 32, 'epochs': 50, 'batch_size': 32}\n",
      "Best RF Parameters: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': None}\n",
      "Best XGB Parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01}\n",
      "Stacking ensemble trained successfully.\n",
      "Ensemble model and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Flatten, Dense, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Do not pass an `input_shape`.*\")\n",
    "# -----------------------------\n",
    "# Load and preprocess the data\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\ML_work\\Data Engineering work\\CNN(Model)_weather_prediction\\uasin_gishu_weather_data_cleaned.csv\", parse_dates=[\"time\"])\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values(by='time')\n",
    "\n",
    "# Features and target\n",
    "features = ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max']\n",
    "target = 'precipitation_sum'\n",
    "\n",
    "# Check for missing values and fill them with column mean\n",
    "df[features + [target]] = df[features + [target]].fillna(df[features + [target]].mean())\n",
    "\n",
    "# Extract input and output\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape input for CNN and LSTM: (samples, timesteps, features)\n",
    "X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# -----------------------------\n",
    "# Define base model functions\n",
    "# -----------------------------\n",
    "# CNN Model\n",
    "def build_cnn_model(filters=64, kernel_size=2, dense_units=32, learning_rate=0.001, input_shape=(1, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# LSTM Model\n",
    "def build_lstm_model(units=50, dense_units=32, learning_rate=0.001, input_shape=(1, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Wrap models for scikit-learn\n",
    "# -----------------------------\n",
    "cnn_regressor = KerasRegressor(\n",
    "    model=build_cnn_model,\n",
    "    model__input_shape=(X_scaled_3d.shape[1], X_scaled_3d.shape[2]),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lstm_regressor = KerasRegressor(\n",
    "    model=build_lstm_model,\n",
    "    model__input_shape=(X_scaled_3d.shape[1], X_scaled_3d.shape[2]),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "xgb_regressor = XGBRegressor(random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameter spaces\n",
    "# -----------------------------\n",
    "cnn_param_dist = {\n",
    "    'model__filters': [32, 64, 128],\n",
    "    'model__kernel_size': [1, 2],\n",
    "    'model__dense_units': [16, 32, 64],\n",
    "    'model__learning_rate': [0.001, 0.0001],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [30, 50]\n",
    "}\n",
    "\n",
    "lstm_param_dist = {\n",
    "    'model__units': [50, 100],\n",
    "    'model__dense_units': [16, 32, 64],\n",
    "    'model__learning_rate': [0.001, 0.0001],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [30, 50]\n",
    "}\n",
    "\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Cross-validation and search\n",
    "# -----------------------------\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Tune CNN\n",
    "cnn_search = RandomizedSearchCV(\n",
    "    estimator=cnn_regressor,\n",
    "    param_distributions=cnn_param_dist,\n",
    "    cv=kfold,\n",
    "    n_iter=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "cnn_search.fit(X_scaled_3d, y, callbacks=[early_stop])\n",
    "print(\"Best CNN Parameters:\", cnn_search.best_params_)\n",
    "\n",
    "# Tune LSTM\n",
    "lstm_search = RandomizedSearchCV(\n",
    "    estimator=lstm_regressor,\n",
    "    param_distributions=lstm_param_dist,\n",
    "    cv=kfold,\n",
    "    n_iter=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "lstm_search.fit(X_scaled_3d, y, callbacks=[early_stop])\n",
    "print(\"Best LSTM Parameters:\", lstm_search.best_params_)\n",
    "\n",
    "# Tune Random Forest\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_regressor,\n",
    "    param_distributions=rf_param_dist,\n",
    "    cv=kfold,\n",
    "    n_iter=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "rf_search.fit(X_scaled, y)  # RF uses 2D input\n",
    "print(\"Best RF Parameters:\", rf_search.best_params_)\n",
    "\n",
    "# Tune XGBoost\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_regressor,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    cv=kfold,\n",
    "    n_iter=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_search.fit(X_scaled, y)  # XGBoost uses 2D input\n",
    "print(\"Best XGB Parameters:\", xgb_search.best_params_)\n",
    "\n",
    "# -----------------------------\n",
    "# Create stacking ensemble\n",
    "# -----------------------------\n",
    "# Custom wrapper to handle 3D input for neural networks\n",
    "class StackingWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model=None, is_3d=False):\n",
    "        self.model = model\n",
    "        self.is_3d = is_3d\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be provided before fitting.\")\n",
    "        if self.is_3d:\n",
    "            X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be provided before predicting.\")\n",
    "        if self.is_3d:\n",
    "            X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {'is_3d': self.is_3d, 'model': self.model}\n",
    "        if deep and self.model is not None and hasattr(self.model, 'get_params'):\n",
    "            params.update({f'model__{k}': v for k, v in self.model.get_params(deep=deep).items()})\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        if 'is_3d' in params:\n",
    "            self.is_3d = params.pop('is_3d')\n",
    "        if 'model' in params:\n",
    "            self.model = params.pop('model')\n",
    "        if self.model is not None and hasattr(self.model, 'set_params'):\n",
    "            self.model.set_params(**params)\n",
    "        return self\n",
    "\n",
    "# Define estimators for stacking\n",
    "estimators = [\n",
    "    ('cnn', StackingWrapper(model=clone(cnn_search.best_estimator_), is_3d=True)),\n",
    "    ('lstm', StackingWrapper(model=clone(lstm_search.best_estimator_), is_3d=True)),\n",
    "    ('rf', clone(rf_search.best_estimator_)),\n",
    "    ('xgb', clone(xgb_search.best_estimator_))\n",
    "]\n",
    "\n",
    "# Define meta-learner\n",
    "meta_learner = LinearRegression()\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=kfold\n",
    ")\n",
    "\n",
    "# Fit the stacking ensemble\n",
    "stacking_regressor.fit(X_scaled, y)\n",
    "print(\"Stacking ensemble trained successfully.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save ensemble model and scaler\n",
    "# -----------------------------\n",
    "# Save the entire stacking ensemble as a single entity\n",
    "joblib.dump(stacking_regressor, \"weather_ensemble_model.pkl\")\n",
    "joblib.dump(scaler, \"weather_scaler.pkl\")\n",
    "\n",
    "print(\"Ensemble model and scaler saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
